"""
DERBi PIE. Not sure what it means, but it's a good name.

The purpose of this project is to scan in and digitize PIE dictionaries.
These can be in several languages, but we start with a german one.

The final site needs a couple of features:
- search
    - search for roots
    - search by words (a word is generated by the roots?) (partial match?)
    - search by meaning
    - search by word class
- view entries laid out in a nice way
    - ????
- likely will be written with nodejs
- something else?

Before that we need to:
- OCR scan PIE dictionaries
    - the OCR stuff likely will need to be post processed somehow, preferably automatically
- come up with a suitable schema for organizing this data (may ask mom for help)
    - we expect the data to be large, allegedly GB of text
- come up with a set of queries for the DB (this can be changed later, but we should think up a couple starter ones in the meantime)
-

Concepts:
- front end: This is the portion of the code that is rendered on the user's computer. mostly about looks
- back end: This is the portion of the code that processes user's requests, assembles the code that is sent over to the user, and holds database stuff.
    - users cant directly query the DB, that would be unsafe obviously
- tesseract: Open Source OCR software. will likely need to be customized or something for the sake of our purposes.
- DERBi PIE: this entire project, but can also be used to refer to the website.
"""
import glob
import json
import numpy as np
from simpledbf import Dbf5


def combine_json():
    ocr_full = []
    for file in glob.glob("indodoc/data/*"):
        with open(file, "r", encoding="utf8") as fp:
            ocr_full += json.load(fp)["responses"]

    # sort by page number since glob reads them out of order
    ocr_full = sorted(ocr_full, key=lambda x: x['context']["pageNumber"])

    # save to disk to avoid doing that again
    output_file = "indodoc/LIV_ocr.npy"
    np.save(output_file, {"ocr_full": ocr_full}, allow_pickle=True)

    return ocr_full, output_file


def load_combined_json(file="indodoc/LIV_ocr.npy"):
    ocr_full = np.load(file, allow_pickle=True).item()["ocr_full"]
    return ocr_full, file


def flatten_pages(ocr_full, page_start=70, page_end=707, page_limit=-1):
    use_pages = list(range(page_start, page_end, 1))
    use_pages = use_pages[0:page_limit]
    # go from page to page attempting to find the "header" of a new entry. then process that entry
    pages = [page for page in ocr_full if page["context"]["pageNumber"] in use_pages]
    # flatten the structure a little bit
    pages = [
        {
            "page_num": page["context"]["pageNumber"],
            "languages": page["fullTextAnnotation"]["pages"][0]["property"]["detectedLanguages"],
            "width": page["fullTextAnnotation"]["pages"][0]["width"],
            "height": page["fullTextAnnotation"]["pages"][0]["height"],
            "confidence": page["fullTextAnnotation"]["pages"][0]["confidence"],
            "blocks": page["fullTextAnnotation"]["pages"][0]["blocks"]
        }
        for page in pages
    ]
    return pages


def main():
    # ocr_full, output_file = combine_json()
    ocr_full, output_file = load_combined_json()

    # we need to manually figure out which pages are the ones we need and which are the ones that can be ignored
    # to start I will make some assumptions
    pages = flatten_pages(ocr_full, page_limit=20)

    breakpoint()
    pass


if __name__ == '__main__':
    main()
    pass
